[{"content":"欢迎来到我的博客 这是使用 Hugo + Docker 部署的第一篇文章！\n","permalink":"https://blog.suyuri.com/hello-world/","summary":"\u003ch2 id=\"欢迎来到我的博客\"\u003e欢迎来到我的博客\u003c/h2\u003e\n\u003cp\u003e这是使用 Hugo + Docker 部署的第一篇文章！\u003c/p\u003e","title":"我的第一篇文章"},{"content":"前言：\n在 Docker 的世界里，IPv6 就像是一个被后妈虐待的孩子。 如果你尝试在自定义 Bridge 网络下开启 IPv6，你大概率会遇到这个极其离谱的现象： 容器刚启动时网络是通的，但过了一会儿就断了。 你在宿主机 ping 它一下，它就醒几秒，不踢它它就装死（FAILED 状态）。 本文将带你彻底根治这个“富贵病”，并顺便批判一下 Docker 烂到骨子里的 IPv6 实现。\n1. 为什么 Docker 的 IPv6 会“装死”呢？ 在标准的 IPv6 协议中，设备之间靠 NDP（邻居发现协议） 来寻找对方。当宿主机想给容器发包时，会广播问一句：“谁是 fd00:1::2？”\nDocker 的罪状有三：\n网桥太“蠢”：Docker 创建的 Linux 网桥经常漏掉组播包，导致容器听不见宿主机的召唤。 邻居表老化：Linux 内核默认只缓存几十秒邻居记录。一旦记录失效，宿主机再次发起探测，如果探测包在虚拟网桥里丢了，邻居表就会变成 FAILED。 缺乏保活机制：Docker 并没有为 Bridge 模式提供稳定的 NDP Proxy 或是保活代理。 结论：在 Docker 这种烂网桥环境下，动态邻居发现是不可靠的。\n2. 终极解法：强制“永久绑定” (Permanent Binding) 既然动态发现不靠谱，我们就把容器的 IP 和 MAC 地址直接焊死在宿主机的邻居表里。\n我们要达到的效果是：将邻居表状态从 REACHABLE（动态）强制提升为 PERMANENT（永久）。一旦变为永久，内核就不会再去发送探测包，连接将永远保持激活。\n为了不让手动操作累死人，我们需要一个自动化守护进程。\n3. 落地实施方案 第一步：固定你的网桥与网段 在 docker-compose.yml 中，必须固定网桥名字，方便脚本识别。\nnetworks: lucky: enable_ipv6: true driver: bridge driver_opts: com.docker.network.bridge.name: br-lucky # 固定网桥名 ipam: config: - subnet: fd00:1:/80 # 你的自定义网段 gateway: fd00:1:1 第二步：编写“守护神”脚本 创建 /usr/local/bin/docker-ipv6-monitor.sh，这个脚本会每 5 秒巡逻一次，发现新容器立刻将其邻居表状态“按死”在永久状态。\n#!/bin/bash # 配置你指定的网桥名 BRIDGE_NAME=\u0026#34;br-lucky\u0026#34; echo \u0026#34;Docker IPv6 Guardian started (Polling Mode)...\u0026#34; while true; do # 确保网桥存在 if [ ! -d \u0026#34;/sys/class/net/$BRIDGE_NAME\u0026#34; ]; then sleep 5 continue fi # 扫描属于该网段的容器 IP 和 MAC docker ps -q | xargs -r docker inspect --format \u0026#39;{{range .NetworkSettings.Networks}}{{.GlobalIPv6Address}} {{.MacAddress}}{{end}}\u0026#39; | grep \u0026#34;^fd00:1\u0026#34; | while read -r ip mac; do if [ ! -z \u0026#34;$ip\u0026#34; ] \u0026amp;\u0026amp; [ ! -z \u0026#34;$mac\u0026#34; ]; then # 核心绝杀：强制写入永久邻居表 (nud permanent) ip -6 neigh replace \u0026#34;$ip\u0026#34; lladdr \u0026#34;$mac\u0026#34; dev \u0026#34;$BRIDGE_NAME\u0026#34; nud permanent fi done sleep 5 done 第三步：解决 Systemd 的“水土不服” 为了防止脚本因为格式或环境问题无法自启，执行以下命令进行“大清洗”：\n# 1. 洗掉可能存在的 Windows 换行符 sudo sed -i \u0026#39;s/\\r$//\u0026#39; /usr/local/bin/docker-ipv6-monitor.sh # 2. 确保 Shebang 正确 sudo sed -i \u0026#39;1c #!/bin/bash\u0026#39; /usr/local/bin/docker-ipv6-monitor.sh # 3. 赋予权限 sudo chmod +x /usr/local/bin/docker-ipv6-monitor.sh 第四步：创建系统服务 (Systemd) 创建 /etc/systemd/system/docker-ipv6-guardian.service：\n[Unit] Description=Docker IPv6 Neighbor Table Guardian After=docker.service Requires=docker.service [Service] Type=simple # 显式调用 bash 防止 203 错误 ExecStart=/bin/bash /usr/local/bin/docker-ipv6-monitor.sh Restart=always RestartSec=5 Environment=\u0026#34;PATH=/usr/bin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\u0026#34; [Install] WantedBy=multi-user.target 4. 运行与验证 sudo systemctl daemon-reload sudo systemctl enable --now docker-ipv6-guardian 运行后，输入 ip -6 neigh show | grep fd00，你会看到满屏的 PERMANENT。\n这时候，你可以试着把容器晾在那一个小时，然后再去访问，你会发现它秒通！再也不需要宿主机去踢它那一脚了。\n希望本文的“暴力美学”解法，能给同样被折磨的你节省一点宝贵的睡眠时间。\n","permalink":"https://blog.suyuri.com/posts/%E6%80%92%E6%80%BC-docker%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E4%BF%AE%E5%A4%8D-bridge-%E6%A8%A1%E5%BC%8F-ipv6-%E4%B8%8D%E8%B8%A2%E4%B8%8D%E5%8A%A8%E7%9A%84%E9%99%88%E5%B9%B4%E9%A1%BD%E7%96%BE/","summary":"\u003cp\u003e\u003cstrong\u003e前言\u003c/strong\u003e：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在 Docker 的世界里，IPv6 就像是一个被后妈虐待的孩子。\n如果你尝试在自定义 Bridge 网络下开启 IPv6，你大概率会遇到这个极其离谱的现象：\n容器刚启动时网络是通的，但过了一会儿就断了。\n你在宿主机 \u003ccode\u003eping\u003c/code\u003e 它一下，它就醒几秒，不踢它它就装死（FAILED 状态）。\n本文将带你彻底根治这个“富贵病”，并顺便批判一下 Docker 烂到骨子里的 IPv6 实现。\u003c/p\u003e","title":"Docker Bridge 模式 IPv6 “不踢不动”的陈年顽疾"},{"content":"前言\n最近在折腾 WSL2 的 Mirrored（镜像网络） 模式。这个模式本该是完美的：WSL 和 Windows 共享 IP，端口互通，简直是开发者的福音。 然而，当我开启 Mihomo (Clash内核) 的 TUN 模式，并把 Docker 容器（比如 Lucky、Alist）放入 Bridge（虚拟网桥） 网络后，噩梦开始了：\n现象：Mihomo TUN 一开，外部（局域网/宿主机）访问容器端口直接断连。 诡异点：容器内部访问公网（翻墙）是正常的。 更诡异点：以前用 Host 模式好好的，一切换到 Bridge 模式就挂。 折腾了一整天，试过改 DNS、改路由表 route-exclude、改 strict-route，统统无效。最后终于通过 iptables 打标找到了终极解法。 根本原因：Mihomo 的“强盗逻辑”与 Docker 的“身份危机” 问题的核心在于 WSL2 镜像模式的网络栈是共享的。\nMihomo 的 TUN 劫持逻辑： Mihomo 开启 TUN 后，会下发一条极霸道的策略路由（ip rule）：\nnot from all iif lo lookup 2022 翻译：只要流量不是从 lo（本机回环）接口进来的，统统给我进代理隧道！\nDocker Bridge 的尴尬：\nHost 模式下：容器共享宿主机网络，流量被内核视为“本机流量”（近似 lo 或 eth0），侥幸逃过 Mihomo 的劫持。 Bridge 模式下：容器流量是从 br-xxxx（虚拟网桥）出来的。在 Mihomo 眼里，这就是“外来流量”，直接被无脑劫持进 TUN。 为什么 route-exclude 没用？ Mihomo 的配置 route-exclude-address 排除的是目的地 IP。\n场景：外部访问 Lucky -\u0026gt; Lucky 回包给外部。 死结：Mihomo 是在入口处（根据接口 iif）就进行了劫持，根本还没来得及看你的出口（目的地 IP）是不是在白名单里，数据包就已经进了黑洞。 解决方案：iptables 端口精准分流 既然 Mihomo 按“接口”一刀切，我们就用 Linux 底层的 iptables 按“端口”做精准手术。\n核心思路： 利用 mangle 表，给特定服务端口（如 16601, 443, 3478）的数据包打上 fwmark（防火墙标记）。然后告诉 Linux 内核：“凡是带这个标记的包，直接走主路由，严禁进入 Mihomo 的代理表”。\n终极一键脚本 这个脚本解决了以下痛点：\nTCP 直连：保证 Web 面板、HTTPS 服务能被外部访问。 UDP 直连：保证 STUN (3478) 能探测真实 IP，保证 Tailscale (41641) 能 P2P 打洞。 自动持久化：解决重启 WSL 规则丢失的问题。 1. 创建脚本 在 WSL 中执行：\nsudo tee /usr/local/bin/fix-lucky-net.sh \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #!/bin/bash # ================= 配置区域 ================= # TCP 端口：Web面板、反代服务 (如 Lucky后台, 443) TCP_PORTS=\u0026#34;16601,8405,8443,443\u0026#34; # UDP 端口：STUN探测、VPN打洞 (如 Tailscale) # 注意：3478 必须直连，否则无法获取真实公网IP UDP_PORTS=\u0026#34;3478,41641\u0026#34; # =========================================== # 1. 清理旧规则（防止重复堆积） ip rule del fwmark 0x1 lookup main priority 8000 2\u0026gt;/dev/null iptables -t mangle -F PREROUTING iptables -t mangle -F OUTPUT # 2. 添加策略路由：见到 0x1 标记，直接查主表(main)，绕过代理 ip rule add fwmark 0x1 lookup main priority 8000 # 3. 【TCP】打标规则 if [ -n \u0026#34;$TCP_PORTS\u0026#34; ]; then # 外部流量进来 (PREROUTING) iptables -t mangle -A PREROUTING -p tcp -m multiport --sports $TCP_PORTS -j MARK --set-mark 0x1 # 本机流量出去 (OUTPUT) iptables -t mangle -A OUTPUT -p tcp -m multiport --sports $TCP_PORTS -j MARK --set-mark 0x1 fi # 4. 【UDP】打标规则 (关键！) if [ -n \u0026#34;$UDP_PORTS\u0026#34; ]; then iptables -t mangle -A PREROUTING -p udp -m multiport --sports $UDP_PORTS -j MARK --set-mark 0x1 iptables -t mangle -A OUTPUT -p udp -m multiport --sports $UDP_PORTS -j MARK --set-mark 0x1 fi # 5. 内核参数修正 (防止开启 TUN 后回包因路径校验被丢弃) sysctl -w net.ipv4.conf.all.rp_filter=0 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 sysctl -w net.ipv4.conf.eth0.rp_filter=0 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 echo \u0026#34;[$(date)] Network Rules Applied. TCP: $TCP_PORTS | UDP: $UDP_PORTS\u0026#34; \u0026gt;\u0026gt; /tmp/lucky-fix.log EOF ","permalink":"https://blog.suyuri.com/posts/tun%E4%BF%AE%E5%A4%8D/","summary":"\u003cp\u003e\u003cstrong\u003e前言\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e最近在折腾 WSL2 的 \u003cstrong\u003eMirrored（镜像网络）\u003c/strong\u003e 模式。这个模式本该是完美的：WSL 和 Windows 共享 IP，端口互通，简直是开发者的福音。\n然而，当我开启 \u003cstrong\u003eMihomo (Clash内核) 的 TUN 模式\u003c/strong\u003e，并把 Docker 容器（比如 Lucky、Alist）放入 \u003cstrong\u003eBridge（虚拟网桥）\u003c/strong\u003e 网络后，噩梦开始了：\u003c/p\u003e","title":"WSL2 镜像网络 + Mihomo TUN + Docker Bridge 共存"},{"content":"前言\n在折腾家庭服务器或 VPS 时，我们经常会遇到一种令人头秃的情况：宿主机本身的 IPv6 访问完全正常，但在 Docker 容器（例如 Lucky 反代、Nginx 等）内部的服务，明明监听了 IPv6 端口，外部却死活 TCPing 不通。 特别是当你还在服务器上运行了 Mihomo (Clash) 等透明代理工具时，这个问题尤为明显。本文将分享一个基于 ip6tables 和策略路由的 Shell 脚本，完美解决 Docker 虚拟网桥下的 IPv6 直连问题。\n🚫 问题现象 宿主机 IPv6 正常，可以 curl -6 google.com，外部也能 ping 通宿主机。\nDocker 容器部署在默认网桥 (docker0) 或自定义网桥（如 br-lucky）上。\n容器端口已映射（例如 -p 80:80）。\n症状：外部通过 IPv6 访问容器端口超时（Time out），TCPing 失败。\n环境：Linux 服务器，运行了 Docker，且很可能运行了类似 Mihomo 的透明代理接管了流量。\n🧐 原因分析 当你在服务器上运行透明代理时，代理程序通常会修改路由表或使用 TProxy/Tun 模式接管进出流量。\n入站流量：外部请求通过 IPv6 到达宿主机网卡（如 eth1），被转发给 Docker 网桥，最终到达容器。\n出站流量（回包）：容器处理完请求准备回包时，数据包从 Docker 网桥出来。\n路由冲突：由于透明代理的规则（通常是策略路由），这个回包可能会被误判为需要“代理”的流量，或者被错误的路由表引导，导致它无法原路返回给外部请求者，从而造成连接中断。\n我们需要做的，就是给“回包”打上标记，让它绕过代理规则，强制走主路由表直连出去。\n🛠️ 解决方案：一键修复脚本 针对这个问题，我编写了一个 Bash 脚本。它主要做了两件事：\n打标（Marking）：给从外部接口进来的连接打上标记（0x114），并确保 Docker 容器回应的数据包继承这个标记。\n策略路由（Policy Routing）：告诉系统，凡是带有 0x114 标记的数据包，直接查 main 路由表，不要走代理的路由表。\n脚本内容 创建一个名为 fix_docker_ipv6.sh 的文件：\nBash\n#!/bin/bash # 确保以 root 权限运行 if [ \u0026#34;$EUID\u0026#34; -ne 0 ]; then echo \u0026#34;请使用 sudo 运行此脚本\u0026#34; exit 1 fi echo \u0026#34;正在配置 Mihomo/Docker IPv6 直连规则...\u0026#34; # =========================== # 1. 配置变量 (请根据实际情况修改) # =========================== # 外网物理网卡名称 (例如 eth0, eth1, enp3s0) WAN_INTERFACE=\u0026#34;eth1\u0026#34; # 自定义 Docker 网桥名称 (Lucky 反代所在的网桥) CUSTOM_BRIDGE=\u0026#34;br-lucky\u0026#34; # 标记值 (16进制) FWMARK=\u0026#34;0x114\u0026#34; # =========================== # 2. 路由策略 (Policy Routing) # =========================== # 清理旧规则 (避免重复) ip -6 rule del fwmark $FWMARK lookup main pref 8900 2\u0026gt;/dev/null # 添加新规则：凡是标记为 0x114 的 IPv6 包，强制查询 main 路由表 ip -6 rule add fwmark $FWMARK lookup main pref 8900 # =========================== # 3. 定义 iptables 辅助函数 # =========================== add_rule() { local chain=$1 shift # check 检查规则是否存在，不存在则 append ip6tables -t mangle -C \u0026#34;$chain\u0026#34; \u0026#34;$@\u0026#34; 2\u0026gt;/dev/null if [ $? -ne 0 ]; then ip6tables -t mangle -A \u0026#34;$chain\u0026#34; \u0026#34;$@\u0026#34; echo \u0026#34;已添加规则: $chain $@\u0026#34; else echo \u0026#34;规则已存在，跳过: $chain $@\u0026#34; fi } # =========================== # 4. 配置 Mangle 表规则 # =========================== # [进站打标] # 当数据包从外网接口进来时，给整个连接 (Connection) 打上标记 add_rule PREROUTING -i $WAN_INTERFACE -j CONNMARK --set-mark $FWMARK # [Docker 查标] # 当数据包从默认 Docker 网桥出来时，将连接的标记恢复到数据包上 add_rule PREROUTING -i docker0 -j CONNMARK --restore-mark # [自定义网桥查标] # 当数据包从 Lucky 网桥出来时，同样恢复标记 # 注意：即使网桥当前未启动，添加此规则也是安全的 add_rule PREROUTING -i $CUSTOM_BRIDGE -j CONNMARK --restore-mark # [宿主机查标] # 宿主机自身发出的包也尝试恢复标记 (用于本机访问) add_rule OUTPUT -j CONNMARK --restore-mark echo \u0026#34;IPv6 修复脚本执行完成。\u0026#34; 关键配置说明 ip -6 rule add fwmark 0x114 lookup main：这是核心。它告诉 Linux 内核，只要看到标记是 0x114 的 IPv6 包，就忽略掉其他的路由表（比如 Clash 创建的路由表），直接去查主路由表，保证数据包能正确扔回给网关。\nCONNMARK --set-mark：在 PREROUTING 链（数据包刚进网卡时）记录标记。\nCONNMARK --restore-mark：在 Docker 容器回包时，把之前记录的标记“贴”回到回程的数据包上。\n🚀 如何使用 保存脚本：将上面的代码保存为 fix_docker_ipv6.sh。\n修改网卡名称：务必检查脚本中的 WAN_INTERFACE=\u0026quot;eth1\u0026quot; 和 CUSTOM_BRIDGE=\u0026quot;br-lucky\u0026quot; 是否与你的实际环境一致。可以使用 ip addr 查看你的网卡名称。\n赋予权限：\nBash\nchmod +x fix_docker_ipv6.sh 运行脚本：\nBash\nsudo ./fix_docker_ipv6.sh 运行后，再次尝试从外部 IPv6 TCPing 你的 Lucky 端口，应该就能通了！\n💡 持久化建议 重启服务器后，iptables 规则和路由策略通常会丢失。建议将此脚本加入开机自启：\n方法一：添加到 /etc/rc.local (如果系统支持)。\n方法二：创建一个简单的 systemd service。\n方法三：如果是 OpenWrt 或特定 NAS 系统，放在对应的“自定义脚本”设置中。\n通过这个简单的脚本，我们利用 Linux 强大的网络标记功能，成功打通了 Docker 容器的 IPv6“任督二脉”，既保留了透明代理的功能，又实现了服务的直连访问。希望这对你有所帮助！\n","permalink":"https://blog.suyuri.com/posts/%E8%A7%A3%E5%86%B3-docker-%E5%AE%B9%E5%99%A8%E5%9C%A8%E9%80%8F%E6%98%8E%E4%BB%A3%E7%90%86%E7%8E%AF%E5%A2%83%E4%B8%8B-ipv6-%E6%97%A0%E6%B3%95%E8%A2%AB%E5%A4%96%E9%83%A8%E8%AE%BF%E9%97%AE%E7%9A%84%E9%97%AE%E9%A2%98/","summary":"\u003cp\u003e\u003cstrong\u003e前言\u003c/strong\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在折腾家庭服务器或 VPS 时，我们经常会遇到一种令人头秃的情况：宿主机本身的 IPv6 访问完全正常，但在 Docker 容器（例如 Lucky 反代、Nginx 等）内部的服务，明明监听了 IPv6 端口，外部却死活 TCPing 不通。\n特别是当你还在服务器上运行了 \u003cstrong\u003eMihomo (Clash)\u003c/strong\u003e 等透明代理工具时，这个问题尤为明显。本文将分享一个基于 \u003ccode\u003eip6tables\u003c/code\u003e 和策略路由的 Shell 脚本，完美解决 Docker 虚拟网桥下的 IPv6 直连问题。\u003c/p\u003e","title":"Docker 容器在透明代理环境下 IPv6 无法被外部访问的问题"}]